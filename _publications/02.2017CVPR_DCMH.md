---
layout: publication
sitemap: false
title: "Deep Cross-Modal Hashing"
authors: <u>Qing-Yuan Jiang</u>, Wu-Jun Li.
pdf: CVPR2017_DCMH
image: CVPR2017_DCMH.jpg
display: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
type: Spotlight
display_short: CVPR
slide: CVPR2017_DCMH_slide.pdf
year: 2017
video: https://www.youtube.com/watch?v=RZKwK-jfcfE
poster: CVPR2017_DCMH_Poster.pdf
learning2hash: true
cross_modal_retrieval: true
github: https://github.com/jiangqy/DCMH-CVPR2017
abstract: "Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity search in multimedia retrieval applications. However, most existing CMH methods are based on hand-crafted features which might not be optimally compatible with the hash-code learning procedure. As a result, existing CMH methods with hand-crafted features may not achieve satisfactory performance. In this paper, we propose a novel CMH method, called deep cross-modal hashing (DCMH), by integrating feature learning and hash-code learning into the same framework. DCMH is an end-to-end learning framework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments on three real datasets with image-text modalities show that DCMH can outperform other baselines to achieve the state-of-the-art performance in cross-modal retrieval applications."
---
